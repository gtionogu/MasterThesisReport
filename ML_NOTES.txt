Web Notes on different project topics:
--------------------------------------

+ Honeynet: FF
	. Because fast-flux techniques utilize blind TCP and UDP redirects, any directional service protocol with a single target port would likely encounter few problems being served via a fast-flux service network
	. Please note that in the figure below that request caching is not taken into account and that the outbound request would usually emanate from the client's preferred nameserver instead of the client itself
	. it’s all about ROI for the criminals, and fast-flux service networks provide a reliable way to maximize the returns on their criminal activities for relatively low effort. Fast-flux service networks offer three major advantages to operators of Internet based criminal activity.
		1. simplicity: Only one suitably powerful backend server (or mothership) host is needed to serve the master content and DNS information. Very simple infra to manage
		2. legal protection: layer of protection from ongoing investigative response or legal action
		3. fast-flux service networks extend the operational lifespan of the critical backend core servers

	SINGLE VS DOUBLE example https://www.honeynet.org/node/138

    1. Establish policies to enable blocking of TCP 80 and UDP 53 into user-land networks if possible (ISP)
    2. Block access to controller infrastructure (motherships, registration, and availability checkers) as they are discovered. (ISP)
    3. Improving domain registrar response procedures, and auditing new registrations for likely fraudulent purpose. (Registrar)
    4. Increase service provider awareness, foster understanding of the threat, shared processes and knowledge. (ISP)
    5. Blackhole DNS and BGP route injection to kill related motherships and management infrastructure. (ISP)
    6. Passive DNS harvesting/monitoring to identify A or NS records advertised into publicly routable user IP space. (ISPs, Registrars, Security professionals, ...)

+ ML 
	. Fine-tuning models
		- only when everything has already been tried
            - EDA https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184
		- improve data https://medium.com/fintechexplained/processing-data-to-improve-machine-learning-models-accuracy-de17c655dc8e
			. How we fill missing or erroneous values?

                FIND DATA: missingno.matrix(train, figsize = (30,10))

                OUTLINERS? Feature engineering can be broken down into three categories: adding, removing and changing.


				data_frame = pd.read_csv(my_data)
				
				1. REMOVING DATA
					
					#remove all missing data
					data_frame.dropna()
					#remove missing data from specific columns
					data_frame.dropna(subset=['TOtal Wealth'])

				2. REPLACE DATA
					
					#set default value
					data_frame = pd.read_csv(my_data)
					#back fill
					data_frame.replacena(method='bfill')
					#front fill
					data_frame.replacena(method='bfill')
					#interpolate by placing mean values using sci-kit learn
					from sklearn.preprocessing import Imputer
					imr = Imputer(missing_values='NaN', strategy='mean', axis=0)
					imr = imr.fit(df)
					imputed_data = imr.transform(data_frame.values)

			. How we need to handle categorical or textual values?

				1. CREATE CATEGORY DICT (good for low number of )

					map = {'Fashion': 1, 'Economical':2}
					#this will map categorical to numerical values
					target_feature = 'Most Active Current News Type'
					data_frame[target_feature] = data_frame[target_feature].map(map)

				2. If large number of categories (>30) -> ENCODERS

					from sklearn.preprocessing import LabelEncoder
					target_feature = 'Most Active Current News Type'
					#use encoder and transform
					encoder = LabelEncoder()
					encoded_values = encoder.fit_transform(data_frame[target_feature].values)
					data_frame[target_feature] = pd.Series(encoded_values, index=data_frame.index)
					#to inverse, use inverse method
					decoded = encoder.inverse_transform(data_frame[target_feature].values)
					data_frame[target_feature] = pd.Series(decoded, index=data_frame.index)

				3. Higher values in categories -> HOT ENCODING

					basically creates a feature for each value of the category

					data_frame = pd.get_dummies(data_frame) # OR data_frame = pd.get_dummies(data_frame)

			. How we need to scale the values? https://www.renom.jp/notebooks/tutorial/preprocessing/normalization/notebook.html

				1. Normalisation

					Normalised Value = (Value - Feature Min)/(Feature Max - Feature Min)
					sklearn.preprocessing.MinMaxScaler can be used to perform normalisation


					from sklearn import preprocessing
					import numpy as np
					# Get dataset
					df = pd.read_csv("https://storage.googleapis.com/mledudatasets/california_housing_train.csv", sep=",")
					# Normalize total_bedrooms column
					x_array = np.array(df['total_bedrooms'])
					normalized_X = preprocessing.normalize([x_array])

					Why would we normalize in the first place?
                        USE THIS IN THE EXPERIMENT
						1. Normalization makes training less sensitive to the scale of features, so we can better solve for coefficients.
						2. The use of a normalization method will improve analysis from multiple models.
						3. Normalizing will ensure that a convergence problem does not have a massive variance, making optimization feasible
				2. Standardisation
                    Standarderised Value = (Value - Feature Mean)/Feature Standard Deviation
					sklearn.preprocessing.StandardScaler can be used to perform standarisation.

					from sklearn import preprocessing
					# Get column names first
					names = df.columns
					# Create the Scaler object
					scaler = preprocessing.StandardScaler()
					# Fit your data on the scaler object
					scaled_df = scaler.fit_transform(df)
					scaled_df = pd.DataFrame(scaled_df, columns=names)

					Why standardize?
						1. Compare features that have different units or scales.
						2. Standardizing tends to make the training process well behaved because the numerical condition of the optimization problems is improved
							/!\ 

                when to std or norm?

                You generally standardize in a multivariate analysis when you want all variables to be in comparable units. Normalize can mean different things. sometimes it means to fit a normal distribution to a set of data or also to transform a variable to a normal variable. Also in some disciplines normalize means to divide a sum or integral by a value that makes the resulting sum or integral one. HTH David

                I. Normalization: data should be normalized if there is any predefined assumption of the model which you are going to use else there is no need. ANN doesn't have any assumption about the data.
                II. Yes I agree with David, you need to standardized your data only when you are dealing with multiple variables together (i.e. Multivarite Anaysis). this is also valid for some other models not for ANN.

                Article about norm vs std 

                https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf

			. How we need to select features?

            /!\ When we are training our models even when we are training imputers or scalars, always use training set to the train models. Leave test or validation set for testing only.


                1. Solve overfitting:
                    A. We can remove features that have strong correlation with each other. You can use correlation matrix to determine correlation between all independent variables.
                    B. We could also use scatter mix plot to determine how all variables are linked to each other.
                    C. Importance 

                        my_importance_model = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)
                        my_importance_model.fit(independent_variables, dependent_variables)
                        print(my_importance_model.feature_importances_)

                        least important can be excluded (can also use data summary)

    . How we need to reduce dimensions? https://medium.com/fintechexplained/what-is-dimension-reduction-in-data-science-2aa5547f4d29
    1. Principal component analysis (PCA), https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/
    2. Linear Discriminant Analysis (LDA) or 
    3. Kernel principal component analysis to reduce the dimensions.

    Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables

    Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.

		- choose the right algorithm 
            1. Bruteforce 
            2. Understanding algorithms and applying most sense for the data

    . Find Score metric 

        Understand metrics: https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b

    QUALITY of predictions https://scikit-learn.org/stable/modules/model_evaluation.html
        - Mean
        - Mode
        - Median
        - Variance
        - Std Deviation
        - Covariance
        - Correlation
            Correlation is always between -1 and 1. -1 indicates that the variables are negatively correlated and +1 shows that the variables are positively correlated. 0 indicates that there is no correlation amongst the target variables.
        - Explained Sum Of Squares
        - Sum Of Squared Residuals
        - Total Sum of Squares
        - R-Squared
        - Adjusted R-Squared
        - Standard Error Of Regression
        - Mean Absolute Error
        - Root Mean Squared Error
        - F1
        - Confusion matrix
            + summary of classification
        - Euclidean Distance
            Finds similarity between two variables
        - Manhattan Distance
            Finds similarity between two variables
        - Minkowski Distance
            Metric form of Euclidean and Manhattan distances
        - Cosine Similarity
            Finds how similar two variables X and Y are


        METRICS: https://turi.com/learn/userguide/evaluation/classification.html REALLY GOOD METRICS PAPER



    . Accurate prediction score
        - Cross Validation
            There are two common cross validation methodologies
            1. Holdout Cross Validation

                A. Train your model on the training set (60% of the data), 
                B. Model selection (tuning parameters) on validation set (20% of the data) and 
                C. test your model on the test set (20% of the data).

            2. K fold cross validation
            
                from sklearn.cross_validation import cross_val_score 
                scores = cross_val_score(estimator=pipe_lr, X=X_train, y=Y_train, cv=12, n_jobs=)
                mean_scores = scores.mean()

        . Best parameters with validation curves
            Before we tune the parameters, we need to diagnose and find if the model is suffering from under or overfitting.
                - Large number of parameters tends to overfit
            OPTIMIZE HYPER PARAMETERS: https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models…

            1. Grid search
                from sklearn.grid_search import GridSearchCV

            2. Random search


MACHINE LEARNING INTERPRETABILITY https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f
---------------------------------

Feature importance:
- Generalised Linear Models
- Random Forest and SVM
- Lime https://github.com/marcotcr/lime

Interpretation: https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739
- Exploratory analysis and visualization techniques like clustering and dimensionality reduction.
    + Dimensionality reduction: Principal Component Analysis (PCA), Self-organizing maps (SOM), Latent Semantic Indexing
    + Manifold Learning: t-Distributed Stochastic Neighbor Embedding (t-SNE)
    + Variational autoencoders: An automated generative approach using variational autoencoders (VAE)
    + Clustering: Hierarchical Clustering
- Model performance evaluation metrics
    + Supervised Learning — Classification: For classification problems, our main objective is to predict a discrete categorical response variable. The confusion matrix is extremely useful here from which we can derive a whole bunch of useful metrics including accuracy, precision, recall, F1-Score as depicted in the following example.
    + ROC curve and the AUC score are really recommended

- Feature Importance
    + Partial Dependence Plots
        * Shap
        * Skater
    
- Global Surrogate Models
    + TreeSurrogates
- Local Interpretable Model-agnostic Explanations (LIME)
- Shapley Values and SHapley Additive exPlanations (SHAP)
    + The ‘game’ is the prediction task for a single instance of the dataset.
    + The ‘gain’ is the actual prediction for this instance minus the average prediction of all instances.
    + The ‘players’ are the feature values of the instance, which collaborate to receive the gain (= predict a certain value).





DNS tunneling
-------------

https://unit42.paloaltonetworks.com/dns-tunneling-how-dns-can-be-abused-by-malicious-actors/

Basic traffic with CnC:
1. HEARTBEAT: 
    request, type A with embedded data(encoded or plaintext) in the query encoded.bad.com
    response, type A with code NXDOMAIN since domain not registered or pretending

2. Exfiltration (ONLY USE OF REQUESTS)

Anything could be sent only limitation: UDP size + UDP best effort
- size can be bypassed with truncation 
- very complicated to deal with dropped datagrams

3. Infiltration (RESPONSES)
- almost no limitations, especially using the TXT DNS record.
- perfect for base64
- we are still unsure if things arrive

When does the bot actually asks for TXT records?
When requesting for the A records most of the time the answer will be NXDOMAINS
with the answer NOERROR, the CnC can use the IPv4 response to embed commands.

Imagine the values returned as commands that can indicate to request TXT RR or to wipe the traces, or any possible malicious command possible.

A lot of instructions can also be hidden in the parameters of the queries, such as the flags. Since the DNS server is under malicious control all the exchanges could be configured and have conditional responses.

https://www.giac.org/paper/gcia/1116/detecting-dns-tunneling/108367

A better way to qualify tunneling is VPN over DNS as a service. Malware installs client and the server decodes the DNS requests and does the proxying of the real traffic. example of tools: Iodine.

The core techniques
used by all DNS tunneling utilities include a controlled domain or subdomain, a server side component, a client side component and data encoded in DNS payloads. Server and client each hosting one side of the tunnel.

- Encoding techniques, this is where tools vary:
    + Data encoded in the dns request.
    + CNAME response by the server possible 
    + Server initiation of communication can be only achieved by regular poll of the server by the client so if the server wants to send data he can respond to the requests.

Type of encoding and type of RR queried are variant. Some tools even detect the best option @iodine. EDNS allows to have larger payloads

Detection techniques:
    + payload analysis
        * size of request and response (Bianco, 2006)
            - ratio of source and destination bytes compared against a threshold.
            - length of dns query and response (Pietraszek, 2004), (Skoudis, 2012)
            - hostname request longer then 52 char (Guy,2009)
        * Entropy of hostname
        * statistical analysis (Bilge, 2011) with LMS and (Guy, 2009) with an alert with anything with above 27 unique characters.
        * Uncommon record types (Pietraszek, 2004)
        * policy violation  (Fry, 2009)
        * specific signatures (Van Horenbeeck, 2006)
    + traffic analysis
        * volume of DNS traffic per IP (Pietraszek, 2004), (Van Horenbeeck, 2006)
        * volume of DNS traffic per domain  (Butler, 2011)
        * number of hostnames per domain (Guy, 2009)
        * geographic location of DNS server (Skoudis, 2012)
        * Domain history (Zrdnja, 2007)
            - when was the record added?
        * volume of NXDomain responses  (Antonakakis, 2012)
        * Visualization  (Guy, 2009).
        * Orphan DNS requests

DNS shadowing
-------------
https://blog.malwarebytes.com/threat-analysis/2015/04/domain-shadowing-with-a-twist/
.Definition: Domain shadowing is a technique where attackers steal domain account credentials from their owners for the purpose of creating sub domains directed at malicious servers (reverse proxy).

This gives criminals a huge amount of URLs they can cycle through and discard after use. This architecture is efficient because it connects victims with malicious URLs that are served to them ‘on-demand’ in a completely automated fashion.

---
For cyber criminals, domain shadowing is creating  thousands of subdomains by generally capturing user information with phishing. it’s shown as a way to prevent classical detection methods like IP or websites blocking because it utilizes legit sites as fronts for malicious delivery.

1. domain owners’ identity information is captured by phishing or keylogging methods.
2. creation of subdomains using the compromised owner's credentials.
3. Angler Exploit Kit begins here. The work is designed to redirect victims to an attacker-controlled webpage hosted on the first tier of subdomains.
4. Users are redirected to the exploit kit landing pages hosted on the second tier of subdomains

Anlger EK: The Angler exploit kit is now one of the best exploit kits on the market. With Zero Day, it offers the ability to quickly and effectively integrate many emerging abuses such as Adobe Flash Player zero-days and Internet Explorer exploits.
++++++
Angler Exploit Kit has begun utilizing these hijacked domain registrant accounts to serve malicious content. This is an increasingly effective attack vector since most individuals don’t monitor their domain registrant accounts regularly

Since this campaign has done an exceptional job of rotation not only the subdomains, but also the IP addresses associated with the campaign.  Additionally, these subdomains are being rotated quickly minimizing the time the exploits are active, further hindering both block list effectiveness and analysis

Domain shadowing is the process of using users domain registration logins to create subdomains

FEATURES: 

There are some other differentiators between the redirecting domains and the exploit domains. The redirecting domains only made use of third level domains that were english word based (i.e. says.imperialsocks.com). The landing page / exploit kit subdomains are random string based and recently have branched into using both third level and fourth level domains (i.e. brandmuellergekwantifiseer.astarentals.co.uk &  3e3qcq.plante.bplawfirm.net)

Detection:

Don’t Let One Rotten Apple Spoil the Whole Barrel:
Towards Automated Detection of Shadowed Domains \cite{shadowing}

3 RR associated with subdomain creation:
    - A
    - CNAME (alias)
    - AAAA
In particular, the bad actors harvested a large amount of credentials
of domain owners (e.g., through phishing emails or brute-force
guessing) and logged into their accounts to create subdomains.
This technique is called domain shadowing, and such subdomains
are called shadowed domains
The malicious subdomains inherit the reputation of legitimate apex domain
Goal: detection of bulk creation of shadowed domains

1. Deviation from legitimate domains under the same apex
2. Ccorrelation amongst shadowed domains under different apex
- subdomain usage 
    + analysis of date between apex creation and subdomdomain non-www creation. 
    + ratio of popular domain (under upper apex and IP???)
    + web connectivity, subdomains are deteched from their apex, there aren't any links + shadow domains cloaked (connectivity checked through web archive)
- subdomain hosting
    + deviation of hosting, distant IPs from apex and good subdomains + different AS
    + correlation ratio, ratio of cohosted subdomains and dc(apex with subdomains hosted together)
- subdomain activity
    + distribution of first seen date
    + resolution count
    + active days
- subdomain name
    + diversity of name levels
    + subdomain name length
    
Detection system: Woodpecker
    a. creates profile of S (subdomains observed)
    b. aggregation of profiles and extracts 17 features.
    c. classifier training


Rank Feature Score rank Feature Score
1 F10 0.26188
2 F2* 0.13213 
3 F7* 0.11509 
4 F17 0.06493 
5 F5* 0.0623 
6 F9 0.05221 
7 F1* 0.04496 
8 F14 0.04424 
9 F11* 0.03451
10 F8* 0.03374
11 F12* 0.03183
12 F16* 0.03128
13 F3* 0.02852
14 F15 0.02395
15 F13* 0.02309
16 F6* 0.01491
17 F4* 0.00036

USE THIS TO DECIDE WHICH FEATURES TO TAKE

Table 5: Importance of features. Features marked with an
asterisk (*) are novel.

Feature analysis
    We assess the importance of our features through a standard metric
in the RandomForest model, namely mean decrease impurity (MDI).

Datasets:

Most manual through google research. Use of PDNS farsight and 360(security companies)
as well as a live feed for VirusTotal totest the model with new data.
## From 360
{"rrname": "eu.account.amazon.com", "rrtype": "A",
"rdata": "52.94.216.25;", "count": 31188,
"time_first": 1477960509, "time_last": 1494290720}
## From Farsight
{"rrname":"aws.amazon.com.","rrtype":"A",
"rdata":["54.240.255.207"], "count":63,
"time_first":1302981660,"time_last":1318508315}

Conclusions:

Shadowed domains. Finally, we analyze the characteristics of
shadowed domains and their apex. Basically, the number of shadowed domains under an apex is quite random, from one up to 2,989
with the average number at six. Most shadowed domains have a
short lifetime and are mostly (85%) resolved for less than five times
per IP. Figure 12 shows the CDF of the active days of shadowed
domains. Among them, 85% are observed for only one day. This
indicates that miscreants rotate shadowed domains quickly, in a
similar fashion as fast-flux networks [39].
Previous work [14] uses the TTL value to identify malicious
domains. We do not use it for our problem, since it is usually not
distinctive on the ground-truth set. We verify this design choice
on the entire set by sending DNS queries for 10,000 randomly
sampled resolvable shadowed domains. The result confirms our
prior observation that the value is either the same as their apex or
within the normal range of other legitimate domains.
By cross-checking with VT, we find that 126,384 shadowed domains were submitted to VirusTotal but only 14,134 subdomains
were alarmed. In other words, security companies have not yet
devised and deployed an effective solution, and we believe that
Woodpecker can provide great value in tackling domain shadowing


FEATURE SELECTION JUSTIFICATION:
In the following paragraphs, the most important features are presented.
It is essential to take the most valuable features in machine
learning because, the more features we have, the more it is likely that
the model will overfit the dataset [19]. Therefore, dropping some
features that do not significantly improve the predictions could help
generalizing the model and produce better results on unknown data.


PAPERS:
-------
Malicious Domain Names Detection Algorithm Based on N-Gram

2 sections: domain name whitelist substring construction and 
            malicious domain names detection

1. Domain Name Whitelist Substring Set Construction
    - substring stats:
        + substrings for each N-gram for each domain
        + n-gam done without Top Level DomainD
        + substring length average
            *   Length         3     4      5     6     7
                Proportion (%) 5.39 20.09 29.13 29.21 13.81
        + duplicate substrings are removed ??
    - substring weight values calculation
        The whitelist is created through statistical analysis of N-grams in legit domains (Alexa). Adding the weight calculation as well using 3-7 N-grams sizes.
        - malicious domain detection
            + Reputation values calculation
                * sum of weights for each gram
            + threshold setting D=0.65 for the study (optimal values)
                * this is the reputation value under which we consider it to be a malicious domain

----------------------------------------

MAKING SURE THE ORDER MAKES SENSE

ML 
ML for detection (classification)
ML workflow
    data gathering (dataset, balancing, labeling)
    data pre-processing
        feature selection(pattern/type/independence of features/large set of features)
        feature extraction



